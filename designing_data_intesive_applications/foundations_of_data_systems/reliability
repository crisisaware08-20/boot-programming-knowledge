Reliability
============

Tolerating software and hardware faults


By reliability is understand the following
   - The application performs the function that user expect
   - It can tolerate the user making mistakes or using the software in unexpected ways
   - Its performance is good enough for the required use case, under the expected load and data volume
   - The system prevents any unauthorized access and abuse


One word - working correctly. It keeps work correctly even when things go wrong
Such systems are said to be resilient or fault tolerance



Faults
==========

Faults --> Can lead to system failure

Faults can be cured
Faults can not be cured


    Hardware Faults
    ================

    Hard disk crash
    RAM becomes faulty
    Unplug wrong network cable

    Mean time to failure - MTTF


    Solutions: Redundancy, fairly common for vm instances to become unavailable without warning
    Systems that tolerate the loss of entire machines by using software fault-tolerance techinques in preference or in addition hardware redundancy
    Such systems also has advantages: a single-server system require planned downtime if you need to reboot the machine, whereas a system  that can tolerate machine failure can be patched one node at a time, without downtime of the entire system


    Sofware Errors
    ================

    Hardware faults are not random and independent, there might be a weak correlation - like hard disk are close to each other
    It can happen that a large number of hardware components will fail at the same time



    Another class of faults are - systematic error within the system. And because they are correlated across nodes, they tend to cause many more system failures than uncorrelated hardware faults

    - A software bug that causes every instance of an application server to crash when a given particular input is given
    - A runawya process that uses up some shared resources - CPU time, memeory, disk space, or network bandwidth
    - A service that the system depends on that slows down, becomes unresponsive
    - Cascading failures

    No quick solution. The risk could be reduced: carefully thinking about assumptions and interactions in the
    system; thorough testing; process isolation; allowing process to crash and restart; measuring; monitoring; and analysing system behavior in production


    Human Errors
    =============

    Design systems in a way that minimize oportunities for error. Well-designed abstractions, APIs, and admin interfaces make it easy to do "the right thing" and discourage the "the wrong thing". Interface too resstrictive .... people will negate their benifit and will work around it. The balance


    Decouple the places where people make the most mistakes from the places where they can cause failures.
    Provide fully featured sandbox environments where people can experiment safely, using real data, without affecting real users.


    Test

    Allow quick and easy recovery from human errors, to minimize the impact in the case of failure.
    Fast to rollback configuration changes, roll out new code gradually(so that any unexpected bugs affect only a small subset of users)

    Set up detailed and clear monitoring, such as performance metrics and error rates. In other engineering disciplines this is referred to as telemetry






















